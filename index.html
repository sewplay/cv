<html>
<head>
    <meta charset="UTF-8">
    <title>Eunwoo Song - CV
    </title>
</head>
<body>

<article>
    <header>
        <h1>Eunwoo Song
    </header>
</article>
gregorio.song@gmail.com

<br><br>

<div>
I received the combined M.S. and Ph.D. degree in electrical and electronic engineering at Yonsei University, Seoul, Korea in 2019. Since Mar. 2017, I have been joining with NAVER Corp., Sungnam, Korea. I served his internships at Microsoft Research Asia, Beijing, China, from Sep. 2015 to Jun. 2016, and Qualcomm Technologies Inc., San-Diego, CA, from Aug. 2016 to Nov. 2016, respectively. My research interests include speech/audio signal processing, speech/audio coding, speech synthesis, and machine learning.
</div>

<hr>

<div>
<b>Research experience</b>
<br>
<ul>
  <li><b>Naver Corp.</b>, Seongnam, Korea</li>
  <ul>
    <li>Mar 2017 - present</li>
    <li>Research Scientist, Clova AI Lab</li>
    <li>Topic: Speech synthesis</li>
    <ul>
      <li>Implementing and evaluating state-of-the-art speech synthesis models, such as Tacotron, Tacotron 2, WaveNet, WaveRNN, WaveGlow. Experimenting on these models by architectural and feature-level modifications.</li>
      <li>Research and development of ExcitNet vocoder, incorporating linear prediction filter to neural vocoder architecture for quality improvement. [<a href=https://sewplay.github.io/demos/excitnet>Demo</a>]</li>
      <li>Research and development of end-to-end expressive speech synthesis system, leveraging global style token-based emotion embedding methods. [<a href=https://sewplay.github.io/demos/gst_tacotron2_excitnet>Demo</a>]</li>
      <li>Research and development of hybrid speech synthesis system, combining deep learning and unit-selection TTS models. Implementing cloud-based real-time TTS products for</li>
      <ul>
        <li>Naver Clova AI speaker (Korean Celeb voice), [<a href=https://clova.ai/ko/events/celeb_voice>Web</a>]</li> 
        <li>Gatebox (Japanese Character voice). [<a href=https://gatebox.ai/home>Web</a>]</li>
      </ul>
    </ul>
  </ul>
</ul>
<ul>
  <li><b>Qualcomm Technologies Inc.</b>, San Diego, CA</li>
  <ul>
    <li>Aug 2016 - Nov 2016</li>
    <li>Mentor: Dr. Deep Sen</li>
    <li>Topic: Spatial audio</li>
    <ul>
      <li>Fixed-point implementation of MPEG-H 3D Audio Decoder</li>
    </ul>
  </ul>
</ul>  
<ul>
  <li><b>Microsoft Research Asia</b>, Beijing, China</li>
  <ul>
    <li>Apr 2016 – Jun 2016</li>
    <li>Sep 2015 – Feb 2016 </li>
    <li>Mentor: Dr. Frank Soong</li>
    <li>Topic: Speech synthesis</li>
    <ul>
      <li>Deep learning-based TTS system using ITFTE vocoder</li>
    </ul>
  </ul>
</ul>
<ul>
  <li><b>Yonsei University</b>, Seoul, Korea</li>
  <ul>
    <li>Sep 2010 – Feb 2017</li>
    <li>Research Assistant for DSP Lab.</li>
  </ul>
</ul>
</div>

<hr>

<div>
<b>Education</b>
<br>
<ul>
  <li><b>Yonsei University</b>, Seoul, Korea</li>
  <ul>
    <li>Combined M.S. and Ph.D., Electrical and Electronic Engineering, Feb 2019</li>
    <ul>
      <li>Dissertation: Improved time-frequency trajectory excitation vocoder for deep learning-based statistical parametric speech synthesis system</li>      
      <li>Advisor: Prof. Hong-Goo Kang</li>
    </ul>
    <li>B.S., Electrical and Electronic Engineering, Aug 2010</li>
  </ul>
</ul>
</div>

<hr>

<div>
<b>Publications</b>
<br>
<ul>
  <li>Probability density distillation with generative adversarial networks for high-quality parallel waveform generation
  [<a href=https://arxiv.org/abs/1904.04472/>Paper</a>]
  [<a href=https://r9y9.github.io/demos/projects/interspeech2019/>Demo</a>]
  <br>Ryuichi Yamamoto, Eunwoo Song, Jae-Min Kim
  <br>Proc. INTERSPEECH, 2019, in press.</li>
  <li>ExcitNet vocoder: A neural excitation model for parametric speech synthesis systems
  [<a href=https://arxiv.org/abs/1811.04769/>Paper</a>]
  [<a href=https://sewplay.github.io/demos/excitnet/>Demo</a>]
  <br>Eunwoo Song, Kyungguen Byun, Hong-Goo Kang
  <br>Proc. EUSIPCO, 2019, in press.</li>
  <li>Excitation-by-SampleRNN model for text-to-speech
  [<a href=papers/2019/itccscc.pdf>Paper</a>]
  <br>Kyungguen Byun, Eunwoo Song, Jinseob Kim, Jae-Min Kim, Hong-Goo Kang
  <br>Proc. ITC-CSCC, 2019, pp. 356-359. </li>
  <li>Acoustic modeling using adversarially trained variational recurrent neural network for speech synthesis
  [<a href=papers/2018/interspeech_1598.pdf>Paper</a>]
  <br>Joun Yeop Lee, Sung Jun Cheon, Byoung Jin Choi, Nam Soo Kim, Eunwoo Song
  <br>Proc. INTERSPEECH, 2018, pp. 917-921.</li>
  <li>A unified framework for the generation of glottal signals in deep learning-based parametric speech synthesis systems
  [<a href=papers/2018/interspeech_1590.pdf>Paper</a>]
  <br>Min-Jae Hwang, Eunwoo Song, J.-S. Kim, Hong-Goo Kang
  <br>Proc. INTERSPEECH, 2018, pp. 912-916.</li>
  <li>Modeling-by-generation-structured noise compensation algorithm for glottal vocoding speech synthesis system
  [<a href=papers/2018/icassp_0005669.pdf>Paper</a>]
  <br>Min-Jae Hwang, Eunwoo Song, Hong-Goo Kang
  <br>Proc. ICASSP, 2018, pp. 5669-5673.</li>
  <li>Perceptual quality and modeling accuracy of excitation parameters in DLSTM-based speech synthesis systems
  [<a href=papers/2017/asru_0000671.pdf>Paper</a>]
  <br>Eunwoo Song, Frank K. Soong, Hong-Goo Kang
  <br>Proc. ASRU, 2017, pp. 671–676.</li>
  <li>Effective spectral and excitation modeling techniques for LSTM-RNN-based speech synthesis systems
  [<a href=papers/2017/aslp_08017571.pdf>Paper</a>]
  <br>Eunwoo Song, Frank K. Soong, Hong-Goo Kang
  <br>IEEE/ACM Trans. Audio, Speech, and Lang. Process., vol. 25, no. 11, pp. 2152–2161, 2017.</li>
  <li>Improved time-frequency trajectory excitation vocoder for DNN-based speech synthesis
  [<a href=papers/2016/interspeech_IS160230.pdf>Paper</a>]
  <br>Eunwoo Song, Frank K. Soong, Hong-Goo Kang
  <br>Proc. INTERSPEECH, 2016, pp. 874–878.</li>
  <li>Multi-class learning algorithm for deep neural network-based statistical parametric speech synthesis
  [<a href=papers/2016/eusipco_1570245860.pdf>Paper</a>]
  <br>Eunwoo Song, Hong-Goo Kang
  <br>Proc. EUSIPCO, 2016, pp. 1951–1955.</li>
  <li>Deep neural network-based statistical parametric speech synthesis system using improved time-frequency trajectory excitation model
  [<a href=papers/2015/interspeech_IS150697.pdf>Paper</a>]
  <br>Eunwoo Song, Hong-Goo Kang
  <br>Proc. INTERSPEECH, 2015, pp. 874–878.</li>
  <li>A constrained two-layer compression technique for ECG waves
  [<a href=papers/2015/embc_07319791.pdf>Paper</a>]
  <br>Kyungguen Byun, Eunwoo Song, H. Sim, H. Lim, Hong-Goo Kang
  <br>Proc. EMBC, 2015, pp. 6130–6133.</li>
  <li>Improved time-frequency trajectory excitation modeling for a statistical parametric speech synthesis system
  [<a href=papers/2015/icassp_0004949.pdf>Paper</a>]
  <br>Eunwoo Song, Young-Sun Joo, Hong-Goo Kang
  <br>Proc. ICASSP, 2015, pp. 4949–4953.</li>
  <li>Fixed-point implementation of MPEG-D unified speech and audio coding decoder
  [<a href=papers/2014/dsp_06900810.pdf>Paper</a>]
  <br>Eunwoo Song, Hong-Goo Kang, Joonil Lee
  <br>Proc. DSP, 2014, pp. 110–113.</li>
  <li>Speech enhancement for pathological voice using time-frequency trajectory excitation modeling
  [<a href=papers/2013/apsipa_06694125.pdf>Paper</a>]
  <br>Eunwoo Song, Jongyoub Ryu, Hong-Goo Kang
  <br>Proc. APSIPA, 2013, pp. 1–4.</li>
</ul>
</div>

<div>
<b>Preprint</b>
<br>
<ul>
  <li>Effective parameter estimation methods for an ExcitNet model in generative text-to-speech systems
  [<a href=https://arxiv.org/abs/1905.08486/>Paper</a>]
  [<a href=https://sewplay.github.io/demos/gst_tacotron2_excitnet/>Demo</a>]
  <br>Ohsung Kwon, Eunwoo Song, Jae-Min Kim, Hong-Goo Kang
  <br>arXiv preprint arXiv:1905.08486, 2019.</li>
  <li>Speaker-adapted neural vocoders for statistical parametric speech synthesis systems in an ultra-small training data condition
  [<a href=https://arxiv.org/abs/1811.03311/>Paper</a>]
  [<a href=https://sewplay.github.io/demos/vocoder_adaptation/>Demo</a>]
  <br>Eunwoo Song, Jinseob Kim, Kyungguen Byun, Hong-Goo Kang
  <br>arXiv preprint arXiv:1811.04472, 2018.</li>
</ul>
</div>







<hr>
Last updated 3 Aug 2019
<hr>

