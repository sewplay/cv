<html>
<head>
    <meta charset="UTF-8">
    <title>Eunwoo Song - CV
    </title>
</head>
<body>

<article>
    <header>
        <h1>Eunwoo Song
    </header>
</article>
gregorio.song@gmail.com
<br>
<a href=CV_EunwooSong.pdf>CV_EunwooSong.pdf</a>
<br><br>

<div>
I received the combined M.S. and Ph.D. degree in electrical and electronic engineering at Yonsei University, Seoul, Korea in 2019. Since Mar. 2017, I have been joining with NAVER Corp., Seongnam, Korea. I served my internships at Microsoft Research Asia, Beijing, China, from Sep. 2015 to Jun. 2016, and Qualcomm Technologies Inc., San-Diego, CA, from Aug. 2016 to Nov. 2016, respectively. My research interests include speech/audio signal processing, speech/audio coding, speech synthesis, and machine learning.
</div>

<hr>

<div>
<b>Research experience</b>
<br>
<ul>
  <li><b>Naver Corp.</b>, Seongnam, Korea</li>
  <ul>
    <li>Mar 2017 - present</li>
    <li>Senior Research Scientist</li>
    <li>HDTS Team Lead, Clova AI (from Mar 2020)</li>
    <li>Topic: Speech synthesis</li>
    <ul>
      <li>Research and development of hybrid speech synthesis system, combining deep learning and unit-selection TTS models. Implementing cloud-based real-time TTS products for</li>
      <ul>
        <li>Naver AI news anchor (Korean Celeb voice, May 2020). [<a href=https://blog.naver.com/clova_ai/221981676372>Web</a>]</li>        
        <li>Gatebox (Japanese Character voice, Oct 2019). [<a href=https://gatebox.ai/home>Web</a>]</li>        
        <li>Line Car Navi (Japanese Navigation, Sep 2019), [<a href=https://carnavi.line.me>Web</a>]</li>
        <li>Naver Maps (Korean Navigation, Sep 2019), </li>
        <li>Naver Clova AI speaker (Korean Celeb voice, Nov 2018), [<a href=https://clova.ai/ko/events/celeb_voice>Web</a>]</li> 
      </ul>
      <li>Research and development of ExcitNet vocoder, incorporating linear prediction filter to WaveNet architecture for quality improvement. [<a href=https://sewplay.github.io/demos/excitnet>Demo</a>]</li>
      <li>Research and development of Parallel WaveGAN vocoder, incorporating generative adversarial network to non-auto-regressive WaveNet generator. [<a href=https://sewplay.github.io/demos/wavegan-pwsl>Demo</a>]</li>
      <li>Research and development of end-to-end expressive speech synthesis system, leveraging global style token-based emotion embedding methods. [<a href=https://sewplay.github.io/demos/gst_tacotron2_excitnet>Demo</a>]</li>
      <li>Implementing and evaluating state-of-the-art speech synthesis models, such as Tacotron, Tacotron 2, Transformer, WaveNet,  WaveRNN, WaveFlow. Experimenting on these models by architectural and feature-level modifications.</li>
      <li>Implementing and evaluating parametric vocoders for speech synthesis back-end, such as ITFTE, WORLD, STRAIGHT, Glottal Vocoder, HNM, MBE, MELP. Experimenting on these vocoders by architectural modifications for TTS.</li>      
    </ul>
  </ul>
</ul>
<ul>
  <li><b>Qualcomm Technologies Inc.</b>, San Diego, CA</li>
  <ul>
    <li>Aug 2016 - Nov 2016</li>
    <li>Mentor: Dr. Deep Sen</li>
    <li>Topic: Spatial audio</li>
    <ul>
      <li>Fixed-point implementation of MPEG-H 3D Audio Decoder</li>
    </ul>
  </ul>
</ul>  
<ul>
  <li><b>Microsoft Research Asia</b>, Beijing, China</li>
  <ul>
    <li>Apr 2016 – Jun 2016</li>
    <li>Sep 2015 – Feb 2016 </li>
    <li>Mentor: Dr. Frank Soong</li>
    <li>Topic: Speech synthesis</li>
    <ul>
      <li>Deep learning-based TTS system using ITFTE vocoder</li>
    </ul>
  </ul>
</ul>
<ul>
  <li><b>Yonsei University</b>, Seoul, Korea</li>
  <ul>
    <li>Sep 2010 – Feb 2017</li>
    <li>Research Assistant for DSP Lab.</li>
  </ul>
</ul>
</div>

<hr>

<div>
<b>Education</b>
<br>
<ul>
  <li><b>Yonsei University</b>, Seoul, Korea</li>
  <ul>
    <li>Combined M.S. and Ph.D., Electrical and Electronic Engineering, Feb 2019</li>
    <ul>
      <li>Dissertation: Improved time-frequency trajectory excitation vocoder for deep learning-based statistical parametric speech synthesis system</li>      
      <li>Advisor: Prof. Hong-Goo Kang</li>
    </ul>
    <li>B.S., Electrical and Electronic Engineering, Aug 2010</li>
  </ul>
</ul>
</div>

<hr>

<div>
<b>Publications</b>
<br>
<ul>
  <li>[C22] High-fidelity Parallel WaveGAN with multi-band harmonic-plus-noise model
  [<a href=https://min-jae.github.io/interspeech2021/>Demo</a>]
  <br>Min-Jae Hwang, Ryuichi Yamamoto, Eunwoo Song, Jae-Min Kim
  <br>Proc. INTERSPEECH, 2021 (in press).</li>
  <li>[C21] LiteTTS: A decoder-free lightweight text-to-wave synthesis based on generative adversarial networks. 
  [<a href=https://dsp136.github.io/2021-04-01-interspeech-samples/>Demo</a>]
  <br>Huu-Kim Nguyen, Kihyuk Jeong, Seyun Um, Min-Jae Hwang, Eunwoo Song, Hong-Goo Kang 
  <br>Proc. INTERSPEECH, 2021 (in press).</li>
  <li>[C20] Parallel waveform synthesis based on generative adversarial networks with voicing-aware conditional discriminators
  [<a href=papers/2021/icassp_0006024.pdf>Paper</a>]
  [<a href=https://r9y9.github.io/demos/projects/icassp2021/>Demo</a>]
  <br>Ryuichi Yamamoto, Eunwoo Song, Min-Jae Hwang, Jae-Min Kim
  <br>Proc. ICASSP, 2021, pp. 6039-6043.</li>
  <li>[C19] TTS-by-TTS: TTS-driven data augmentation for fast and high-quality speech synthesis
  [<a href=papers/2021/icassp_0006583.pdf>Paper</a>]
  [<a href=https://min-jae.github.io/icassp2021/>Demo</a>]
  <br>Min-Jae Hwang, Ryuichi Yamamoto, Eunwoo Song, Jae-Min Kim
  <br>Proc. ICASSP, 2021, pp. 6598-6602.</li>
  <li>[W3] Improved Parallel WaveGAN with perceptually weighted spectrogram loss
  [<a href=papers/2021/slt_0000470.pdf>Paper</a>]
  [<a href=https://sewplay.github.io/demos/wavegan-pwsl/>Demo</a>]
  <br>Eunwoo Song, Ryuichi Yamamoto, Min-Jae Hwang, Jin-Seob Kim, Ohsung Kwon, Jae-Min Kim
  <br>Proc. SLT, 2021, pp. 470-476.</li>
  <li>[C18] LP-WaveNet: Linear prediction-based WaveNet speech synthesis
  [<a href=papers/2020/apsipa_09306362.pdf>Paper</a>]
  [<a href=https://min-jae.github.io/apsipa2020/>Demo</a>]
  <br>Min-Jae Hwang, Frank Soong, Eunwoo Song, Xi Wang, Hyeonjoo Kang, Hong-Goo Kang
  <br>Proc. APSIPA, 2020, pp. 810-814.</li>
  <li>[C17] ExcitGlow: Improving a WaveGlow-based neural vocoder with linear prediction analysis
  [<a href=papers/2020/apsipa_0000831.pdf>Paper</a>]
  <br>Suhyeon Oh, Hyungseob Lim, Kyungguen Byun, Min-Jae Hwang, Eunwoo Song, Hong-Goo Kang
  <br>Proc. APSIPA, 2020,  pp. 831-836.</li>  
  <li>[C16] Neural text-to-speech with a modeling-by-generation excitation vocoder
  [<a href=papers/2020/interspeech_2116.pdf>Paper</a>]
  [<a href=https://sewplay.github.io/demos/mbg_excitnet/>Demo</a>]
  <br>Eunwoo Song, Min-Jae Hwang, Ryuichi Yamamoto, Jin-Seob Kim, Ohsung Kwon, Jae-Min Kim
  <br>Proc. INTERSPEECH, 2020, pp. 3570-3574.</li>
  <li>[W2] Speaker-adaptive neural vocoders for parametric speech synthesis systems
  [<a href=papers/2020/mmsp_111.pdf>Paper</a>]
  [<a href=https://sewplay.github.io/demos/vocoder_adaptation/>Demo</a>]
  <br>Eunwoo Song, Jinseob Kim, Kyungguen Byun, Hong-Goo Kang
  <br>Proc. MMSP, 2020, pp. 1-5.</li>
  <li>[C15] Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram
  [<a href=papers/2020/icassp_0006194.pdf>Paper</a>]
  [<a href=https://r9y9.github.io/demos/projects/icassp2020/>Demo</a>]
  <br>Ryuichi Yamamoto, Eunwoo Song, Jae-Min Kim
  <br>Proc. ICASSP, 2020, pp. 6194-6198.</li>
  <li>[C14] Improving LPCNet-based text-to-speech with linear predictions-structured mixture density network
  [<a href=papers/2020/icassp_0007214.pdf>Paper</a>]
  [<a href=https://min-jae.github.io/icassp2020/>Demo</a>]
  <br>Min-Jae Hwang, Eunwoo Song, Ryuichi Yamamoto, Frank K. Soong, Hong-Goo Kang
  <br>Proc. ICASSP, 2020, pp. 7214-7218.</li>
  <li>[C13] Probability density distillation with generative adversarial networks for high-quality parallel waveform generation
  [<a href=papers/2019/interspeech_pWaveNet.pdf>Paper</a>]
  [<a href=https://r9y9.github.io/demos/projects/interspeech2019/>Demo</a>]
  <br>Ryuichi Yamamoto, Eunwoo Song, Jae-Min Kim
  <br>Proc. INTERSPEECH, 2019, pp. 699-703.</li>
  <li>[C12] ExcitNet vocoder: A neural excitation model for parametric speech synthesis systems
  [<a href=papers/2019/eusipco_PID5978469.pdf>Paper</a>]
  [<a href=https://sewplay.github.io/demos/excitnet/>Demo</a>]
  <br>Eunwoo Song, Kyungguen Byun, Hong-Goo Kang
  <br>Proc. EUSIPCO, 2019, pp. 1179-1183.</li>
  <li>[C11] Excitation-by-SampleRNN model for text-to-speech
  [<a href=papers/2019/itccscc.pdf>Paper</a>]
  <br>Kyungguen Byun, Eunwoo Song, Jinseob Kim, Jae-Min Kim, Hong-Goo Kang
  <br>Proc. ITC-CSCC, 2019, pp. 356-359. </li>
  <li>[C10] Acoustic modeling using adversarially trained variational recurrent neural network for speech synthesis
  [<a href=papers/2018/interspeech_1598.pdf>Paper</a>]
  <br>Joun Yeop Lee, Sung Jun Cheon, Byoung Jin Choi, Nam Soo Kim, Eunwoo Song
  <br>Proc. INTERSPEECH, 2018, pp. 917-921.</li>
  <li>[C9] A unified framework for the generation of glottal signals in deep learning-based parametric speech synthesis systems
  [<a href=papers/2018/interspeech_1590.pdf>Paper</a>]
  <br>Min-Jae Hwang, Eunwoo Song, J.-S. Kim, Hong-Goo Kang
  <br>Proc. INTERSPEECH, 2018, pp. 912-916.</li>
  <li>[C8] Modeling-by-generation-structured noise compensation algorithm for glottal vocoding speech synthesis system
  [<a href=papers/2018/icassp_0005669.pdf>Paper</a>]
  <br>Min-Jae Hwang, Eunwoo Song, Hong-Goo Kang
  <br>Proc. ICASSP, 2018, pp. 5669-5673.</li>
  <li>[W1] Perceptual quality and modeling accuracy of excitation parameters in DLSTM-based speech synthesis systems
  [<a href=papers/2017/asru_0000671.pdf>Paper</a>]
  <br>Eunwoo Song, Frank K. Soong, Hong-Goo Kang
  <br>Proc. ASRU, 2017, pp. 671–676.</li>
  <li>[J1] Effective spectral and excitation modeling techniques for LSTM-RNN-based speech synthesis systems
  [<a href=papers/2017/aslp_08017571.pdf>Paper</a>]
  <br>Eunwoo Song, Frank K. Soong, Hong-Goo Kang
  <br>IEEE/ACM Trans. Audio, Speech, and Lang. Process., vol. 25, no. 11, pp. 2152–2161, 2017.</li>
  <li>[C7] Improved time-frequency trajectory excitation vocoder for DNN-based speech synthesis
  [<a href=papers/2016/interspeech_IS160230.pdf>Paper</a>]
  <br>Eunwoo Song, Frank K. Soong, Hong-Goo Kang
  <br>Proc. INTERSPEECH, 2016, pp. 874–878.</li>
  <li>[C6] Multi-class learning algorithm for deep neural network-based statistical parametric speech synthesis
  [<a href=papers/2016/eusipco_1570245860.pdf>Paper</a>]
  <br>Eunwoo Song, Hong-Goo Kang
  <br>Proc. EUSIPCO, 2016, pp. 1951–1955.</li>
  <li>[C5] Deep neural network-based statistical parametric speech synthesis system using improved time-frequency trajectory excitation model
  [<a href=papers/2015/interspeech_IS150697.pdf>Paper</a>]
  <br>Eunwoo Song, Hong-Goo Kang
  <br>Proc. INTERSPEECH, 2015, pp. 874–878.</li>
  <li>[C4] A constrained two-layer compression technique for ECG waves
  [<a href=papers/2015/embc_07319791.pdf>Paper</a>]
  <br>Kyungguen Byun, Eunwoo Song, H. Sim, H. Lim, Hong-Goo Kang
  <br>Proc. EMBC, 2015, pp. 6130–6133.</li>
  <li>[C3] Improved time-frequency trajectory excitation modeling for a statistical parametric speech synthesis system
  [<a href=papers/2015/icassp_0004949.pdf>Paper</a>]
  <br>Eunwoo Song, Young-Sun Joo, Hong-Goo Kang
  <br>Proc. ICASSP, 2015, pp. 4949–4953.</li>
  <li>[C2] Fixed-point implementation of MPEG-D unified speech and audio coding decoder
  [<a href=papers/2014/dsp_06900810.pdf>Paper</a>]
  <br>Eunwoo Song, Hong-Goo Kang, Joonil Lee
  <br>Proc. DSP, 2014, pp. 110–113.</li>
  <li>[C1] Speech enhancement for pathological voice using time-frequency trajectory excitation modeling
  [<a href=papers/2013/apsipa_06694125.pdf>Paper</a>]
  <br>Eunwoo Song, Jongyoub Ryu, Hong-Goo Kang
  <br>Proc. APSIPA, 2013, pp. 1–4.</li>
</ul>
</div>

<div>
<b>Preprint</b>
<br>
<ul>
  <li>[P1] Effective parameter estimation methods for an ExcitNet model in generative text-to-speech systems
  [<a href=https://arxiv.org/abs/1905.08486/>Paper</a>]
  [<a href=https://sewplay.github.io/demos/gst_tacotron2_excitnet/>Demo</a>]
  <br>Ohsung Kwon, Eunwoo Song, Jae-Min Kim, Hong-Goo Kang
  <br>arXiv preprint arXiv:1905.08486, 2019.</li>
</ul>
</div>

<hr>

<div>
<b>Patents</b>
<br>
<ul>
  <li>	KR10-2198598, “Method for generating synthesized speech signal, neural vocoder, and training method thereof,” Dec. 2020.</li>
  <li> KR10-2198597, “Neural vocoder and training method of neural vocoder for constructing speaker-adaptive model,” Dec. 2020.</li>
</ul>
</div>

<hr>

<div>
<b>Honors & Awards</b>
<br>
<ul>
  <li>Ranked No. 2 in N Innovation Award 2020, Naver Corp., Dec 2020</li>
  <li>The Best Paper Award, APSIPA ASC 2020, Dec 2020</li>
  <li>Ranked No. 1 in N Innovation Award 2019, Naver Corp., Dec 2019</li>
  <li>Ranked No. 1 in N Innovation Award 2018, Naver Corp., Nov 2018</li>
  <li>Excellent intern award, Microsoft Research Asia, Jun 2016</li>
  <li>Excellent intern award, Microsoft Research Asia, Feb 2016</li>
  <li>Full scholarship, Yonsei University, Mar 2016 - Aug 2010</li>
</ul>
</div>




<hr>
Last updated 24 Jul 2021
<hr>

